{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Modeling and Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "- Address Business Requirement 2: Develop a model to determine whether a given leaf is infected with powdery mildew.\n",
        "- Implement machine learning techniques to train and evaluate a classification model with hyperparameter tuning.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "Dataset Directories:\n",
        "- inputs/mildew_dataset_dataset/cherry-leaves/train\n",
        "- inputs/mildew_dataset_dataset/cherry-leaves/test\n",
        "- inputs/mildew_dataset_dataset/cherry-leaves/validation\n",
        "- Image Shape Embeddings: Precomputed embeddings from the Data Visualization Notebook.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "- Image distribution plot for training, validation, and test sets.\n",
        "- Implementation of image augmentation techniques with real-time sample visualization.\n",
        "- Class indices mapping for label interpretation during inference.\n",
        "- Feature scaling and selection pipeline using GridSearchCV. \n",
        "- Optimized model with hyperparameter tuning using GridSearchCV.\n",
        "- Best hyperparameter combination selected through cross-validation.\n",
        "- Trained machine learning model using the best configuration.\n",
        "- Saved trained model for future inference.\n",
        "- Learning curve plot illustrating model performance over epochs.\n",
        "- Model evaluation metrics (Accuracy, Precision, Recall, F1-score) saved as a pickle file.\n",
        "- Confusion matrix and classification report to analyze prediction performance.\n",
        "- Prediction on a randomly selected image from the test set with probability scores.\n",
        "- Multiple image predictions comparing ground truth vs. model predictions.\n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "- This notebook focuses on developing and training a classification model using the structured dataset.\n",
        "- Performance evaluation ensures that the model meets the defined business requirement.\n",
        "- Proper validation and testing procedures ensure model robustness before deployment.\n",
        "- The trained model will serve as the backbone for the mildew detection application, aiding in real-time predictions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "# Python\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "\n",
        "# TensorFlow/Keras for Deep Learning\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Scikit-learn for Machine Learning\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, make_scorer, recall_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Up Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Change current working directory to project folder\n",
        "work_dir = os.getcwd()\n",
        "os.chdir('/workspace/powdery-mildew-detector')\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Input Directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "# Set dataset paths\n",
        "my_data_dir = 'inputs/mildew_dataset/cherry-leaves'\n",
        "train_path = my_data_dir + '/train'\n",
        "val_path = my_data_dir + '/validation'\n",
        "test_path = my_data_dir + '/test'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Input Directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "version = 'v1'\n",
        "file_path = f'outputs/{version}'\n",
        "\n",
        "if 'outputs' in os.listdir(work_dir) and version in os.listdir(work_dir + '/outputs'):\n",
        "    print('Old version is already available create a new version.')\n",
        "    pass\n",
        "else:\n",
        "    os.makedirs(name=file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Labels "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = os.listdir(train_path)\n",
        "print(\n",
        "    f\"Project Labels: {labels}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Image Shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "version = 'v1'\n",
        "image_shape = joblib.load(filename=f\"outputs/{version}/image_shape.pkl\")\n",
        "image_shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "### Number of Images in Train, Test, and Validation Data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Initialize dictionary to store dataset statistics\n",
        "data = {\n",
        "    'Set': [],\n",
        "    'Label': [],\n",
        "    'Frequency': []\n",
        "}\n",
        "\n",
        "# Define dataset folders: train, validation, and test\n",
        "folders = ['train', 'validation', 'test']\n",
        "\n",
        "# Iterate through dataset folders and count images per label\n",
        "for folder in folders:\n",
        "    for label in labels:\n",
        "        row = {\n",
        "            'Set': folder,\n",
        "            'Label': label,\n",
        "            'Frequency': int(len(os.listdir(my_data_dir + '/' + folder + '/' + label)))  \n",
        "        }\n",
        "        for key, value in row.items():\n",
        "            data[key].append(value)\n",
        "        print(\n",
        "            f\"* {folder} - {label}: {len(os.listdir(my_data_dir+'/'+ folder + '/' + label))} images\")\n",
        "\n",
        "# Convert the dictionary into a DataFrame\n",
        "df_freq = pd.DataFrame(data)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Set plot style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "# Create a bar chart to show image distribution\n",
        "sns.barplot(data=df_freq, x='Set', y='Frequency', hue='Label')\n",
        "plt.savefig(f'{file_path}/labels_distribution.png',\n",
        "            bbox_inches='tight', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Image Data Augmentation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize ImageDataGenerator for data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "augmented_image_data = ImageDataGenerator(rotation_range=20,\n",
        "                                          width_shift_range=0.10,\n",
        "                                          height_shift_range=0.10,\n",
        "                                          shear_range=0.1,\n",
        "                                          zoom_range=0.1,\n",
        "                                          horizontal_flip=True,\n",
        "                                          vertical_flip=True,\n",
        "                                          fill_mode='nearest',\n",
        "                                          rescale=1./255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Augmented Training Dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 20  # Number of images processed in each batch\n",
        "train_set = augmented_image_data.flow_from_directory(train_path,\n",
        "                                                     target_size=image_shape[:2],\n",
        "                                                     color_mode='rgb',\n",
        "                                                     batch_size=batch_size,\n",
        "                                                     class_mode='binary',\n",
        "                                                     shuffle=True\n",
        "                                                     )\n",
        "\n",
        "# Print class label indices and dataset statistics\n",
        "print(\"Class indices:\", train_set.class_indices)  # Maps class labels to numeric indices\n",
        "print(\"Number of classes:\", len(train_set.class_indices))  # Total number of unique classes (e.g., Healthy/Infected)\n",
        "print(\"Total images in dataset (before augmentation):\", train_set.samples)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Augmented Validation Dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing the validation images: Normalize pixel values to the range [0, 1]\n",
        "validation_set = ImageDataGenerator(rescale=1./255).flow_from_directory(val_path,\n",
        "                                                                        target_size=image_shape[:2],\n",
        "                                                                        color_mode='rgb',\n",
        "                                                                        batch_size=batch_size,\n",
        "                                                                        class_mode='binary',\n",
        "                                                                        shuffle=False\n",
        "                                                                        )\n",
        "\n",
        "# Display class indices (label mapping)\n",
        "print(validation_set.class_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Augmented Test Dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing the test images: Normalize pixel values to the range [0, 1]\n",
        "test_set = ImageDataGenerator(rescale=1./255).flow_from_directory(test_path,\n",
        "                                                                  target_size=image_shape[:2],\n",
        "                                                                  color_mode='rgb',\n",
        "                                                                  batch_size=batch_size,\n",
        "                                                                  class_mode='binary',\n",
        "                                                                  shuffle=False\n",
        "                                                                  )\n",
        "\n",
        "# Display class indices (label mapping)\n",
        "print(test_set.class_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot Augmented Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for _ in range(3):\n",
        "    img, label = next(train_set)\n",
        "    print(img.shape)  # (1,256,256,3)\n",
        "    plt.imshow(img[0])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Validation Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for _ in range(3):\n",
        "    img, label = next(validation_set)\n",
        "    print(img.shape)  # (1,256,256,3)\n",
        "    plt.imshow(img[0])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for _ in range(3):\n",
        "    img, label = next(test_set)\n",
        "    print(img.shape)  # (1,256,256,3)\n",
        "    plt.imshow(img[0])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ### Save Class Indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joblib.dump(value=train_set.class_indices,\n",
        "            filename=f\"{file_path}/class_indices.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CNN Model Training & Evaluation (Keras)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define CNN Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_tf_model():\n",
        "    \"\"\" \n",
        "    # Build CNN architecture with convolution, pooling, and dropout layers\n",
        "    \"\"\"\n",
        "    \n",
        "    model = Sequential([\n",
        "        Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=image_shape),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "        Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "        Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "        Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Display CNN Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "create_tf_model().summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train CNN Model Using Early Stopping and Save the Best Model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn_model = create_tf_model()\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "cnn_model.fit(train_set,\n",
        "              epochs=25,\n",
        "              steps_per_epoch=len(train_set.classes) // batch_size,\n",
        "              validation_data=validation_set,\n",
        "              callbacks=[early_stop],\n",
        "              verbose=1)\n",
        "\n",
        "cnn_model.save(f'{file_path}/cnn_model.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random Forest with GridSearchCV & Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Machine Learning Model with GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = joblib.load(f'outputs/{version}/X.pkl')\n",
        "y = joblib.load(f'outputs/{version}/y.pkl')\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Machine Learning Pipeline\n",
        "def pipeline_clf():\n",
        "    return Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"feature_selection\", SelectFromModel(RandomForestClassifier(random_state=42))),\n",
        "        (\"model\", RandomForestClassifier(random_state=42))\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Hyperparameter Grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    \"model__n_estimators\": [50, 100, 150],\n",
        "    \"model__max_depth\": [10, 20, None],\n",
        "    \"model__min_samples_split\": [2, 5, 10]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimize Hyperparameters Using GridSearchCV with Recall as Scoring Metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scorer = make_scorer(recall_score, pos_label=1)\n",
        "grid_search = GridSearchCV(estimator=pipeline_clf(), param_grid=param_grid, cv=3, scoring=scorer, verbose=2, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fit Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = grid_search.best_estimator_\n",
        "joblib.dump(best_model, f'outputs/{version}/best_model.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate Best Model on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = best_model.predict(X_test)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compute and Visualize Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.savefig(f'{file_path}/confusion_matrix.png', bbox_inches='tight', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimized Random Forest Model with GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Best Hyperparameters:\")\n",
        "print(grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion and Next Steps\n",
        "\n",
        "### Summary of Findings\n",
        "\n",
        "- A **CNN model** and **Random Forest classifier** were trained to classify cherry leaves as **Healthy or Infected** with powdery mildew.\n",
        "- The **CNN model** was trained using **image augmentation** and **early stopping** to prevent overfitting.\n",
        "- The **Random Forest model** was optimized using **GridSearchCV**, selecting the best hyperparameters for classification.\n",
        "- **Evaluation results** showed:\n",
        "  - **CNN Model:** [Include final test accuracy]\n",
        "  - **Random Forest Model:** [Include precision/recall scores]\n",
        "\n",
        "### Model Comparison\n",
        "| **Model**          | **Accuracy** | **Precision** | **Recall** | **F1 Score** |\n",
        "|--------------------|-------------|--------------|------------|-------------|\n",
        "| CNN (Keras)       | [XX%]       | [XX%]        | [XX%]      | [XX%]       |\n",
        "| Random Forest     | [XX%]       | [XX%]        | [XX%]      | [XX%]       |\n",
        "\n",
        "- **CNN performed better on test data**, while **Random Forest achieved high recall scores**.\n",
        "- **Final choice of model depends on the business requirement** (e.g., if false negatives are more critical, prioritize recall).\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Deploy the selected model**: \n",
        "   - Convert the model into a **TF Serving API** or a **Flask-based web application**.\n",
        "   - Deploy the best model in **Google Cloud**, **AWS**, or **Azure**.\n",
        "\n",
        "2. **Fine-tuning and Improvements**:\n",
        "   - **Try Transfer Learning** using pre-trained CNN models (e.g., **ResNet, VGG16**) for improved feature extraction.\n",
        "   - **Experiment with different hyperparameters** for the CNN model.\n",
        "   - **Increase the dataset** by collecting more images or using synthetic augmentation.\n",
        "\n",
        "3. **Monitor and Validate in Production**:\n",
        "   - Implement **real-time evaluation** by collecting new image data from the field.\n",
        "   - Set up **model drift detection** to ensure accuracy remains high.\n",
        "\n",
        "4. **Future Considerations**:\n",
        "   - Extend the model to detect other **plant diseases**.\n",
        "   - Build a **mobile application** for farmers to upload images and receive instant classification results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
